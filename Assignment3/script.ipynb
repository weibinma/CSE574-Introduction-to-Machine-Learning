{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_error_blr:  0.02062556400652329\n",
      "\n",
      " train_error_blr:  0.02071294577137169\n",
      "\n",
      " train_error_blr:  0.06280477163815648\n",
      "\n",
      " train_error_blr:  0.0755166637301917\n",
      "\n",
      " train_error_blr:  0.0442572674594386\n",
      "\n",
      " train_error_blr:  0.08168637840353371\n",
      "\n",
      " train_error_blr:  0.03454490617473333\n",
      "\n",
      " train_error_blr:  0.043831522256309305\n",
      "\n",
      " train_error_blr:  0.10996929922813548\n",
      "\n",
      " train_error_blr:  0.09686647290809093\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output: \n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_features + 1, 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    #add bias item for input data\n",
    "    X0 = np.ones((n_data, 1)) #N x 1\n",
    "#     print('shape of X0: ', X0.shape)\n",
    "    new_train_data = np.hstack((X0, train_data)) #N x D+1\n",
    "    yn = labeli #N x 1\n",
    "    #compute theta\n",
    "    z = np.dot(new_train_data, initialWeights) #N x 1\n",
    "    theta = sigmoid(z) #let theta value in (0,1)   #(N, ) \n",
    "    theta = theta.reshape(theta.shape[0], 1) #(N,1)\n",
    "    #compute error\n",
    "    error = -(1 / n_data) * (np.sum((yn * np.log(theta)) + (1 - yn) * np.log(1 - theta))) #we don't need to compute likelihood.\n",
    "    #compute gradient error\n",
    "    error_grad = (1 / n_data) * (np.sum(((theta - yn) * new_train_data), axis=0)).T #(N,1)*(N,D+1)=(N,D+1) \n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W \n",
    "     of Logistic Regression\n",
    "     \n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "         \n",
    "     Output: \n",
    "         label: vector of size N x 1 representing the predicted label of \n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    #add bias term for input data\n",
    "    X0 = np.ones((data.shape[0], 1))\n",
    "    input_data = np.hstack((X0, data))\n",
    "    #compute predicted label\n",
    "    z = np.dot(input_data,W) # w = [w0,w1,...,wD], data = [1,x1,x2,...,xD]\n",
    "    pred = sigmoid(z)  #(N,10) ->choose the max probability of entry in each row, each entry probability is in (0,1).\n",
    "    label_maxindex = np.argmax(pred, axis = 1)  #N\n",
    "    \n",
    "    label = np.reshape(label_maxindex, (data.shape[0], 1))\n",
    "    return label\n",
    "\n",
    "\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights_b: the weight vector of size (D + 1) x 10\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "    \n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "    \n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    #add bias item for input data\n",
    "    X0 = np.ones((n_data, 1))\n",
    "    new_train_data = np.hstack((X0, train_data)) #(N, D+1)\n",
    "    #compute wk\n",
    "    wk = params.reshape((n_feature + 1, n_class)) #(D+1, 10)\n",
    "    #compute posterior probabilities\n",
    "    vector = np.dot(new_train_data, wk) #(N, 10)\n",
    "    post_prob = np.exp(vector) / np.reshape(np.sum(np.exp(vector), axis=1), (n_data, 1))   #(N, 10)\n",
    "#     print((labeli * np.log(post_prob)).shape) #(N, 1)*(N, 10) = (N, 10)\n",
    "    #compute error\n",
    "    error = -np.sum(labeli * np.log(post_prob))#(N, 10).sum -> total error in only one scala value.\n",
    "########### compute total error of each category\n",
    "    train_error_mlr = []\n",
    "    for i in range(n_class):\n",
    "        error_mlr = -np.sum(labeli[:, i] * np.log(post_prob[:,i]))\n",
    "        train_error_mlr.append(error_mlr)\n",
    "###########          \n",
    "    #compute gradient error\n",
    "    for i in range(n_class):\n",
    "        error_grad[:, i] = np.sum((post_prob[:, i] - np.transpose(labeli[:, i])) * np.transpose(new_train_data), axis=1)\n",
    "    error_grad = error_grad.flatten()\n",
    "\n",
    "    return error, error_grad, train_error_mlr\n",
    "\n",
    "\n",
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    #add bias item for input data\n",
    "    X0 = np.ones((data.shape[0], 1))\n",
    "    input_data = np.hstack((X0, data))\n",
    "    #compute predicted label\n",
    "    z = np.dot(input_data, W)\n",
    "    pred = np.exp(z) / np.reshape(np.sum(np.exp(z), axis = 1), (data.shape[0], 1)) #(N,10) ->choose the max one of each row\n",
    "    label_maxindex = np.argmax(pred, axis = 1)  #N\n",
    "    label = np.reshape(label_maxindex, (data.shape[0], 1))\n",
    "    \n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "n_class = 10 # number of classes\n",
    "n_train = train_data.shape[0] # number of classes\n",
    "n_feature = train_data.shape[1] # number of features\n",
    "\n",
    "######compute total error of training data for each category\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    labeli = Y[:, i].reshape(n_train, 1)\n",
    "    args = (train_data, labeli)\n",
    "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
    "    #compute total error of training data with respect to each category\n",
    "    error, error_grad = blrObjFunction(W[:, i], *args)\n",
    "    print('\\n train_error_blr: ', error)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training error in mlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_error_mlr:  [503.77379245715895, 627.6773122382641, 1640.814965978841, 1600.404478485907, 1115.3849518299317, 1758.2481442012422, 698.7854563388089, 1112.9571035254132, 1745.727377769515, 1572.0684136988375]\n"
     ]
    }
   ],
   "source": [
    "# FOR EXTRA CREDIT ONLY\n",
    "n_class = 10 # number of classes\n",
    "n_train = train_data.shape[0] # number of classes\n",
    "n_feature = train_data.shape[1] # number of features\n",
    "######compute total error of training data for each category\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "error_mlr, error_grad_mlr, train_error_mlr = mlrObjFunction(W_b, *args_b)\n",
    "print('\\n train_error_mlr: ', train_error_mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data in blr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:92.742%\n",
      "\n",
      " Validation set Accuracy:91.53999999999999%\n",
      "\n",
      " Testing set Accuracy:92.01%\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy on Training Dataset\n",
    "predicted_label = blrPredict(W, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = blrPredict(W, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = blrPredict(W, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data in mlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:93.112%\n",
      "\n",
      " Validation set Accuracy:92.39%\n",
      "\n",
      " Testing set Accuracy:92.54%\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test error in blr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test_error_blr:  0.025969725707206567\n",
      "\n",
      " test_error_blr:  0.02290759144938041\n",
      "\n",
      " test_error_blr:  0.07267805840620196\n",
      "\n",
      " test_error_blr:  0.07151132476989247\n",
      "\n",
      " test_error_blr:  0.051102537456837106\n",
      "\n",
      " test_error_blr:  0.08464830954311611\n",
      "\n",
      " test_error_blr:  0.03775557630140572\n",
      "\n",
      " test_error_blr:  0.053979648189005344\n",
      "\n",
      " test_error_blr:  0.11276250613315576\n",
      "\n",
      " test_error_blr:  0.10217055554644983\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_class = 10 # number of features\n",
    "n_test = test_data.shape[0] # number of features\n",
    "n_feature = test_data.shape[1] # number of features\n",
    "\n",
    "######compute total error of training data for each category\n",
    "Y = np.zeros((n_test, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (test_label == i).astype(int).ravel()\n",
    "\n",
    "# Logistic Regression with Gradient Descent\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    labeli_test = Y[:, i].reshape(n_test, 1)\n",
    "    args_test = (test_data, labeli_test)\n",
    "    #compute total error of training data with respect to each category\n",
    "    error_test, error_grad = blrObjFunction(W[:, i], *args_test)\n",
    "    print('\\n test_error_blr: ', error_test)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test error in mlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test_error_mlr:  [69.18157290263284, 118.78492930024363, 400.5346028064311, 272.59705148931, 221.2825040256135, 383.3118682762621, 180.7790320284797, 300.989824923036, 391.7513910352269, 336.5870310371406]\n"
     ]
    }
   ],
   "source": [
    "# FOR EXTRA CREDIT ONLY\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "Y = np.zeros((n_test, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (test_label == i).astype(int).ravel()\n",
    "    \n",
    "args_b = (test_data, Y)\n",
    "error_mlr, error_grad_mlr, test_error_mlr = mlrObjFunction(W_b, *args_b)\n",
    "print('\\n test_error_mlr: ', test_error_mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------SVM-------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maweibin/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " train_accuracy of SVM using linear kernel: 99.64%\n",
      " validation_accuracy of SVM using linear kernel: 91.4%\n",
      " test_accuracy of SVM using linear kernel: 92.0%\n",
      "\n",
      " train_accuracy of SVM using rbf and gamma=1 kernel: 100.0%\n",
      " validation_accuracy of SVM using rbf and gamma=1 kernel: 10.2%\n",
      " test_accuracy of SVM using rbf and gamma=1 kernel: 11.700000000000001%\n",
      "\n",
      " train_accuracy of SVM using rbf and gamma=auto kernel: 92.29%\n",
      " validation_accuracy of SVM using rbf and gamma=auto kernel: 92.0%\n",
      " test_accuracy of SVM using rbf and gamma=auto kernel: 92.9%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=1 kernel: 92.29%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=1 kernel: 92.0%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=1 kernel: 92.9%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=10 kernel: 96.34%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=10 kernel: 94.39999999999999%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=10 kernel: 94.89999999999999%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=20 kernel: 97.65%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=20 kernel: 94.65%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=20 kernel: 95.05%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=30 kernel: 98.35000000000001%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=30 kernel: 94.69999999999999%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=30 kernel: 95.55%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=40 kernel: 98.8%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=40 kernel: 94.75%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=40 kernel: 95.65%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=50 kernel: 99.1%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=50 kernel: 94.55%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=50 kernel: 95.45%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=60 kernel: 99.33%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=60 kernel: 94.45%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=60 kernel: 95.5%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=70 kernel: 99.48%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=70 kernel: 94.45%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=70 kernel: 95.55%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=80 kernel: 99.62%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=80 kernel: 94.25%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=80 kernel: 95.5%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=90 kernel: 99.72999999999999%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=90 kernel: 94.19999999999999%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=90 kernel: 95.45%\n",
      "\n",
      " train_accuracy of SVM using rbf, gamma=auto and C=100 kernel: 99.77000000000001%\n",
      " validation_accuracy of SVM using rbf, gamma=auto and C=100 kernel: 94.19999999999999%\n",
      " test_accuracy of SVM using rbf, gamma=auto and C=100 kernel: 95.39999999999999%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script for Support Vector Machine\n",
    "\"\"\"\n",
    "\n",
    "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "#choose 10000 rows randomly of dataset\n",
    "index1 = np.random.choice(train_data.shape[0], int(0.2*train_data.shape[0]), replace=False)  \n",
    "X_train_data = train_data[index1]\n",
    "y_train_data = train_label[index1]\n",
    "index2 = np.random.choice(validation_data.shape[0], int(0.2*validation_data.shape[0]), replace=False)  \n",
    "X_validation_Data = validation_data[index2]\n",
    "y_validation_Data = validation_label[index2]\n",
    "index3 = np.random.choice(test_data.shape[0], int(0.2*test_data.shape[0]), replace=False)  \n",
    "X_test_data = test_data[index3]\n",
    "y_test_data = test_label[index3]\n",
    "######linear kernel\n",
    "#fitting SVM into training, validation, test data\n",
    "##training data\n",
    "classifier1 = SVC(kernel='linear', random_state = 501)\n",
    "classifier1.fit(X_train_data, y_train_data) \n",
    "y_pred1 = classifier1.predict(X_train_data)\n",
    "acc1 = accuracy_score(y_pred1, y_train_data)*100\n",
    "print('\\n train_accuracy of SVM using linear kernel: ' + str(acc1) + '%')\n",
    "##validation data\n",
    "y_pred2 = classifier1.predict(X_validation_Data)\n",
    "acc2 = accuracy_score(y_pred2, y_validation_Data)*100\n",
    "print(' validation_accuracy of SVM using linear kernel: ' + str(acc2) + '%')\n",
    "##test data\n",
    "y_pred3 = classifier1.predict(X_test_data)\n",
    "acc3 = accuracy_score(y_pred3, y_test_data)*100\n",
    "print(' test_accuracy of SVM using linear kernel: ' + str(acc3) + '%')\n",
    "\n",
    "######rbf with gamma=1\n",
    "#fitting SVM into training, validation, test data\n",
    "##training data\n",
    "classifier2 = SVC(kernel='rbf', gamma=1, random_state = 501)\n",
    "classifier2.fit(X_train_data, y_train_data) \n",
    "y_pred11 = classifier2.predict(X_train_data)\n",
    "acc11 = accuracy_score(y_pred11, y_train_data)*100\n",
    "print('\\n train_accuracy of SVM using rbf and gamma=1 kernel: ' + str(acc11) + '%')\n",
    "##validation data\n",
    "y_pred22 = classifier2.predict(X_validation_Data)\n",
    "acc22 = accuracy_score(y_pred22, y_validation_Data)*100\n",
    "print(' validation_accuracy of SVM using rbf and gamma=1 kernel: ' + str(acc22) + '%')\n",
    "##test data \n",
    "y_pred33 = classifier2.predict(X_test_data)\n",
    "acc33 = accuracy_score(y_pred33, y_test_data)*100\n",
    "print(' test_accuracy of SVM using rbf and gamma=1 kernel: ' + str(acc33) + '%')\n",
    "\n",
    "######rbf with gamma=default\n",
    "#fitting SVM into training, validation, test data\n",
    "##training data\n",
    "classifier3 = SVC(kernel='rbf', gamma='auto', random_state = 501)\n",
    "classifier3.fit(X_train_data, y_train_data) \n",
    "y_pred111 = classifier3.predict(X_train_data)\n",
    "acc111 = accuracy_score(y_pred111, y_train_data)*100\n",
    "print('\\n train_accuracy of SVM using rbf and gamma=auto kernel: ' + str(acc111) + '%')\n",
    "##validation data\n",
    "y_pred222 = classifier3.predict(X_validation_Data)\n",
    "acc222 = accuracy_score(y_pred222, y_validation_Data)*100\n",
    "print(' validation_accuracy of SVM using rbf and gamma=auto kernel: ' + str(acc222) + '%')\n",
    "##test data\n",
    "y_pred333 = classifier3.predict(X_test_data)\n",
    "acc333 = accuracy_score(y_pred333, y_test_data)*100\n",
    "print(' test_accuracy of SVM using rbf and gamma=auto kernel: ' + str(acc333) + '%')\n",
    "\n",
    "######rbf with gamma=default,C = 1,10,20,30,...,100 and plot graph of accuracy\n",
    "C = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for i in C:\n",
    "    ##training data\n",
    "    classifier = SVC(kernel='rbf', gamma='auto', C=i, random_state = 501)\n",
    "    classifier.fit(X_train_data, y_train_data) \n",
    "    y_pred1111 = classifier.predict(X_train_data)\n",
    "    acc_c1 = accuracy_score(y_pred1111, y_train_data)*100\n",
    "    print('\\n train_accuracy of SVM using rbf, gamma=auto and C=' + str(i) + ' kernel: ' + str(acc_c1) + '%')\n",
    "    ##validation data\n",
    "    y_pred2222 = classifier.predict(X_validation_Data)\n",
    "    acc_c2 = accuracy_score(y_pred2222, y_validation_Data)*100\n",
    "    print(' validation_accuracy of SVM using rbf, gamma=auto and C=' + str(i) + ' kernel: ' + str(acc_c2) + '%')\n",
    "    ##test data\n",
    "    y_pred3333 = classifier.predict(X_test_data)\n",
    "    acc_c3 = accuracy_score(y_pred3333, y_test_data)*100\n",
    "    print(' test_accuracy of SVM using rbf, gamma=auto and C=' + str(i) + ' kernel: ' + str(acc_c3) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
